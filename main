import os
import re
import time
import requests
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup

def clean_text(text):
    """æ¸…ç†æ–‡æœ¬ï¼šå»é™¤å¤šä½™ç©ºç™½ã€ç‰¹æ®Šå­—ç¬¦ç­‰"""
    text = re.sub(r'\s+', ' ', text)  # åˆå¹¶å¤šä¸ªç©ºç™½ç¬¦
    text = text.strip()
    return text

def extract_main_content(soup):
    """
    å°è¯•æå–ç½‘é¡µæ­£æ–‡å†…å®¹ã€‚
    ç­–ç•¥ï¼šä¼˜å…ˆæ‰¾ <article>, <main>, æˆ–åŒ…å«å¤§é‡æ®µè½çš„ <div>
    """
    # æ–¹æ³•1ï¼šæ‰¾ article æˆ– main æ ‡ç­¾
    for tag in ['article', 'main']:
        main = soup.find(tag)
        if main:
            return '\n\n'.join(p.get_text() for p in main.find_all('p'))
    
    # æ–¹æ³•2ï¼šæ‰¾åŒ…å«æœ€å¤š <p> çš„ divï¼ˆå¸¸è§äºåšå®¢/å°è¯´ç«™ï¼‰
    divs = soup.find_all('div')
    best_div = None
    max_p_count = 0
    for div in divs:
        p_count = len(div.find_all('p'))
        if p_count > max_p_count:
            max_p_count = p_count
            best_div = div
    
    if best_div and max_p_count >= 1:
        return '\n\n'.join(p.get_text() for p in best_div.find_all('p'))
    
    # æ–¹æ³•3ï¼šå…œåº•ï¼šç›´æ¥å–æ‰€æœ‰ p
    return '\n\n'.join(p.get_text() for p in soup.find_all('p'))

def fetch_page(url, timeout=10):
    """å®‰å…¨åœ°è·å–ç½‘é¡µå†…å®¹"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0 Safari/537.36'
        }
        resp = requests.get(url, headers=headers, timeout=timeout)
        resp.raise_for_status()
        resp.encoding = resp.apparent_encoding
        return resp.text
    except Exception as e:
        print(f"âš ï¸ æ— æ³•æŠ“å– {url}: {e}")
        return None

def get_chapter_links(base_url, html):
    """ä»ç›®å½•é¡µæå–æ‰€æœ‰ç« èŠ‚é“¾æ¥"""
    soup = BeautifulSoup(html, 'lxml')
    base_parsed = urlparse(base_url)
    links = []

    # æå–æ‰€æœ‰ a æ ‡ç­¾
    for a in soup.find_all('a', href=True):
        href = a['href']
        full_url = urljoin(base_url, href)
        # åªä¿ç•™åŒåŸŸåä¸‹çš„é“¾æ¥ï¼ˆé¿å…è·³è½¬åˆ°å¤–éƒ¨ç½‘ç«™ï¼‰
        if urlparse(full_url).netloc == base_parsed.netloc:
            title = clean_text(a.get_text())
            if title:  # å¿½ç•¥ç©ºæ ‡é¢˜
                links.append((title, full_url))
    
    # å»é‡ï¼ˆä¿ç•™é¡ºåºï¼‰
    seen = set()
    unique_links = []
    for title, url in links:
        if url not in seen:
            seen.add(url)
            unique_links.append((title, url))
    
    return unique_links

def web_to_book(book_url, output_file="book.txt"):
    print(f"ğŸ“– å¼€å§‹ç”Ÿæˆä¹¦ç±ï¼š{book_url}")
    
    # 1. è·å–ç›®å½•é¡µ
    index_html = fetch_page(book_url)
    if not index_html:
        print("âŒ æ— æ³•è·å–ç›®å½•é¡µ")
        return
    
    # 2. æå–ç« èŠ‚é“¾æ¥
    chapters = get_chapter_links(book_url, index_html)
    if not chapters:
        print("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•ç« èŠ‚é“¾æ¥ï¼Œå°è¯•ç›´æ¥æå–å½“å‰é¡µå†…å®¹...")
        soup = BeautifulSoup(index_html, 'lxml')
        content = extract_main_content(soup)
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(content)
        print(f"âœ… å·²ä¿å­˜å•é¡µå†…å®¹åˆ° {output_file}")
        return

    print(f"ğŸ“š å…±æ‰¾åˆ° {len(chapters)} ä¸ªç« èŠ‚")

    # 3. æŠ“å–æ¯ä¸ªç« èŠ‚
    book_content = []
    for i, (title, url) in enumerate(chapters, 1):
        print(f"  [{i}/{len(chapters)}] æ­£åœ¨æŠ“å–: {title[:30]}...")
        html = fetch_page(url)
        if not html:
            continue
        soup = BeautifulSoup(html, 'lxml')
        content = extract_main_content(soup)
        if content.strip():
            book_content.append(f"\n\nç¬¬{i}ç«  {title}\n{'='*40}\n")
            book_content.append(content)
        time.sleep(0.5)  # ç¤¼è²Œçˆ¬å–ï¼Œé¿å…è¢«å°

    # 4. ä¿å­˜åˆ°æ–‡ä»¶
    full_text = "\n".join(book_content)
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(full_text)
    
    print(f"âœ… ä¹¦ç±å·²ç”Ÿæˆï¼å…± {len(book_content)//2} ç« ï¼Œä¿å­˜ä¸ºï¼š{os.path.abspath(output_file)}")

# ======================
# ä½¿ç”¨ç¤ºä¾‹
# ======================
if __name__ == "__main__":
    url = input("è¯·è¾“å…¥ä¹¦ç±ç›®å½•é¡µçš„ç½‘å€ï¼š").strip()
    if not url.startswith(('http://', 'https://')):
        url = 'https://' + url
    output_name = "my_book.txt"
    web_to_book(url, output_name)
